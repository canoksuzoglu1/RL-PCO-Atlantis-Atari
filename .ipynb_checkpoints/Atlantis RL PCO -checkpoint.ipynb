{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f08cc2c1",
   "metadata": {},
   "source": [
    "## 1.Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fe8984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import gym  \n",
    "from stable_baselines3 import PPO  \n",
    "from stable_baselines3.common.vec_env import VecFrameStack  \n",
    "from stable_baselines3.common.evaluation import evaluate_policy \n",
    "from stable_baselines3.common.env_util import make_atari_env \n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfe3aee",
   "metadata": {},
   "source": [
    "## 2.Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3e39bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and configure the Atari environment\n",
    "env = make_atari_env('ALE/Atlantis-v5', n_envs=4, seed=0)  \n",
    "env = VecFrameStack(env, n_stack=4)  # Stack frames to provide temporal context to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0def63ba",
   "metadata": {},
   "source": [
    "## 3.Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64df4ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path for logging TensorBoard information\n",
    "log_path = os.path.join('Training', 'Logs')\n",
    "\n",
    "# Initialize the PPO model with a CNN policy\n",
    "model = PPO('CnnPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91735da",
   "metadata": {},
   "source": [
    "## 4.Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544d6145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the PPO model\n",
    "model.learn(total_timesteps=2000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50489d91",
   "metadata": {},
   "source": [
    "## 5.Model Saving and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a7dcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path for saving the trained model\n",
    "ppo_path = os.path.join('Training', 'Saved Models', 'PPO_Atlantis_2MT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cfc704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to the specified path\n",
    "model.save(ppo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf9f137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up by deleting the current model instance\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320f64f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model from the saved path\n",
    "model = PPO.load(ppo_path, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943dc574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize the environment for evaluation with a single instance\n",
    "env = make_atari_env('ALE/Atlantis-v5', n_envs=1, seed=0)  # Evaluation requires only one environment instance\n",
    "env = VecFrameStack(env, n_stack=4)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0301bdc",
   "metadata": {},
   "source": [
    "## 6. Evaluation and Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551fdfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of the trained model\n",
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f611bdea",
   "metadata": {},
   "source": [
    "## 7.Real-Time Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7494e2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import time library for real-time rendering\n",
    "import time\n",
    "\n",
    "# Reset the environment and start the loop for real-time gameplay\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    # Predict the next action based on the current observation\n",
    "    action, _states = model.predict(obs)\n",
    "    # Take the action and receive the next observation, reward, and done flag\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    # Render the environment to visualize the gameplay\n",
    "    env.render('human')\n",
    "    # Pause briefly to slow down the rendering loop\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce56cebb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
